{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ml-assignment11.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "13T5JltowOVjg4dHo7kqDIq6h1X86PtWc",
      "authorship_tag": "ABX9TyMmpy//Ky3rC/HRtDpacnxb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nbkwon/ml-assignment/blob/master/ml_assignment11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWAWKGt7oxi3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we now need pyplot for plotting data\n",
        "import matplotlib.pyplot as pyplot\n",
        "def show_plot(download_file = False):\n",
        "    pyplot.legend()\n",
        "    pyplot.show()\n",
        "    if download_file == False:\n",
        "       return\n",
        "    \n",
        "    import google.colab\n",
        "    file_name = \"test.png\"\n",
        "    pyplot.savefig(file_name)\n",
        "    google.colab.files.download(file_name)\n",
        "\n",
        "def make_plot(plot_data_list, use_this = None):\n",
        "    # it's too samll. make bigger.\n",
        "    if use_this is None:\n",
        "        pyplot.figure(figsize=(8, 8))\n",
        "    else:\n",
        "        # use_this.set_figheight(8)\n",
        "        # use_this.set_figwidth(8)\n",
        "        pass\n",
        "\n",
        "    for data_dic, plot_type, color, label in plot_data_list:\n",
        "        if plot_type is \"plot\":\n",
        "            if use_this is None:\n",
        "                pyplot.plot(list(data_dic.keys()), list(data_dic.values()), c=color, label=label)\n",
        "            else:\n",
        "                use_this.plot(list(data_dic.keys()), list(data_dic.values()), c=color, label=label)    \n",
        "        if plot_type is \"scatter\":\n",
        "            if use_this is None:\n",
        "                pyplot.scatter(list(data_dic.keys()), list(data_dic.values()), c=color, label=label)\n",
        "            else:\n",
        "                use_this.scatter(list(data_dic.keys()), list(data_dic.values()), c=color, label=label)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0yDRHQB8qU9",
        "colab_type": "code",
        "outputId": "aaf26aa3-a5d4-4f83-a0b5-edb0e86416dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.datasets import load_files\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "print(\"Start Load Data\")\n",
        "review_data = load_files(r\"movie_review\")\n",
        "X, y = review_data.data, review_data.target\n",
        "\n",
        "documents = []\n",
        "\n",
        "stemmer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "for sen in range(0, len(X)):\n",
        "    # Remove all the special characters\n",
        "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
        "    \n",
        "    # remove all single characters\n",
        "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
        "    \n",
        "    # Remove single characters from the start\n",
        "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
        "    \n",
        "    # Substituting multiple spaces with single space\n",
        "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
        "    \n",
        "    # Removing prefixed 'b'\n",
        "    document = re.sub(r'^b\\s+', '', document)\n",
        "\n",
        "    \n",
        "    # Converting to Lowercase\n",
        "    document = document.lower()\n",
        "    \n",
        "    # Lemmatization\n",
        "    document = document.split()\n",
        "    document = [stemmer.lemmatize(word) for word in document]\n",
        "    document = ' '.join(document)\n",
        "    \n",
        "    documents.append(document)\n",
        "\n",
        "max_features = 1500\n",
        "\n",
        "vectorizer = CountVectorizer(max_features=max_features, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
        "X = vectorizer.fit_transform(documents).toarray()\n",
        "\n",
        "tfidfconverter = TfidfTransformer()\n",
        "X = tfidfconverter.fit_transform(X).toarray()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Start Load Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vs4eRPA6DiZI",
        "colab_type": "code",
        "outputId": "f361e8e1-2644-404d-f53c-b7dec323bbfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "from functools import reduce\n",
        "import cupy as cp\n",
        "\n",
        "def check_convergence(loss, loss_after):\n",
        "    # float 형의 자료형 동일한지 판단을 위해 numpy의 isclose를 사용.\n",
        "    # atol = 0.0000000000000000001\n",
        "    return abs(np.sum(loss - loss_after)) < 0.000000001\n",
        "    # return np.isclose(loss, loss_after)\n",
        "    # return numpy.isclose(theta_list, new_theta_list)\n",
        "\n",
        "\n",
        "def sigmoid_one(value):\n",
        "    return 1 / (1 + np.exp(-value))\n",
        "    \n",
        "def sigmoid(value):\n",
        "    r = 1 / (1 + cp.exp(-value))\n",
        "    return r\n",
        "\n",
        "def vstack(f):\n",
        "    bias = cp.ones((1, f.shape[1]))\n",
        "    return cp.vstack((f, bias))\n",
        "\n",
        "def npmul(a, b):\n",
        "    # 2차원에 한해서 dot과 matmul은 동일함.\n",
        "    return cp.matmul(a,b)\n",
        "\n",
        "def get_by_mat(mat_list, factor_list):\n",
        "    result_list = []\n",
        "\n",
        "    factors = factor_list\n",
        "    result_list.append(factors)\n",
        "\n",
        "    # mat 196 x 785\n",
        "    # mat 784(+1) * 6000\n",
        "    # -> 196 * 6000 \n",
        "    for mat in mat_list:\n",
        "        factors_with_bias = vstack(factors)\n",
        "     \n",
        "        factors = npmul(mat, factors_with_bias)\n",
        "        factors = sigmoid(factors)\n",
        "        result_list.append(factors)\n",
        "\n",
        "    \n",
        "    return result_list\n",
        "\n",
        "def log(n):\n",
        "    float_min = 0.00000000000000001\n",
        "    n[n<float_min] = float_min\n",
        "    n = cp.log(n)\n",
        "    \n",
        "    return n\n",
        "\n",
        "def get_ret_list(mat_list, f_list):\n",
        "    ret = get_by_mat(mat_list, f_list)\n",
        "    return ret\n",
        "\n",
        "def get_theta_count(mat_list):\n",
        "    count = 0\n",
        "    for m in mat_list:\n",
        "        count += m.shape[0] * m.shape[1]\n",
        "    return count\n",
        "\n",
        "def calc_energy_value(mat_list, f_list, d_list, ret_list, lamb):\n",
        "    #d_list = [0 0 0 0 1 0 0 0 0 0]\n",
        "    #ret_list = [[0....0]\n",
        "    # for r_in_d, ret in zip(d_list, ret_list[-1]):\n",
        "    ret = ret_list[-1]\n",
        "    # print(d_list[0], ret[0])\n",
        "    # print((1 - d_list) * log(1 - ret))\n",
        "    calculated = -d_list * log(ret) - (1 - d_list) * log(1 - ret)\n",
        "    calculated /= len(d_list)\n",
        "\n",
        "    loss = cp.sum(calculated)\n",
        "    theta_loss = 0\n",
        "    for m in mat_list:\n",
        "        theta_loss += cp.sum(m ** 2)\n",
        "\n",
        "    theta_loss *= (lamb / (2 * get_theta_count(mat_list)))\n",
        "    # return calculated\n",
        "    # print(calculated)\n",
        "\n",
        "    return loss + theta_loss\n",
        "\n",
        "def calc_next_gradient(lr, mat_list, f_list, d_list, ret_list, lamb):\n",
        "    h_list = ret_list[-1]\n",
        "    calculated_error = (h_list - d_list) \n",
        "    #     calculated_error = (-d_list/h_list) + (1-d_list) * (1 / (1 - h_list)\n",
        "    # calculated_error *= h_list * (1 - h_list )  \n",
        "\n",
        "    error_list = calculated_error  / len(d_list)\n",
        "\n",
        "    new_mat_list = []\n",
        "    theta_count = get_theta_count(mat_list)\n",
        "\n",
        "    for i in range(len(mat_list)-1, -1, -1):\n",
        "        now_mat = mat_list[i]\n",
        "        new_mat = now_mat.copy()\n",
        "\n",
        "        factor_with_bias = vstack(ret_list[i])\n",
        "        # print(factor_with_bias.shape)\n",
        "        # print(error_list.shape)\n",
        "        calced = lr * (npmul(error_list,factor_with_bias.transpose()) + (lamb / theta_count) * now_mat)\n",
        "        # print(\"new_mat calced\", new_mat.shape, calced.shape)\n",
        "        new_mat -= calced\n",
        "\n",
        "        new_mat_list.append(new_mat)\n",
        "\n",
        "        # error_list 갱신. 레이어의 에러를 미리 계산하는거라서 끝난상황이면 갱신할 필요가 없다.\n",
        "        if(i - 1 < 0):\n",
        "            continue\n",
        "        \n",
        "        v = ret_list[i]\n",
        "        v = v * (1 - v)\n",
        "        a = cp.dot(now_mat.transpose(), error_list)\n",
        "        # print(\"check dLdz dzdz(i-1) (i)\", v.shape, factor_with_bias.shape)\n",
        "        \n",
        "        # 49 * 60 -> 50 * 60\n",
        "        new_error_list = v * a[:-1, ]\n",
        "        # print(\"new error_list\", new_error_list.shape)\n",
        "        error_list = new_error_list\n",
        "    \n",
        "    # print(\"new_mat_list len\", len(new_mat_list))\n",
        "    new_mat_list.reverse()\n",
        "\n",
        "    return new_mat_list\n",
        "\n",
        "def calc_accuracy(ret_list, answer_list):\n",
        "    calc = cp.copy(ret_list[-1][0])\n",
        "    ans = cp.copy(answer_list)\n",
        "    calc[calc<0.5] = 0\n",
        "    calc[calc>=0.5] = 1\n",
        "    ans[ans<0.5] = 0\n",
        "    ans[ans>=0.5] = 1\n",
        "    corrected = cp.sum(calc == ans)\n",
        "    \n",
        "    count = len(answer_list)\n",
        "\n",
        "    # print(\"accuracy count\", count)\n",
        "    # print(calculated)\n",
        "    # print(answer)       \n",
        "    # print(corrected)\n",
        "\n",
        "    return  corrected / count\n",
        "\n",
        "# print(list_label)\n",
        "\n",
        "def make_thetas(x, y):\n",
        "    a = cp.random.normal(0, 1, x * y).reshape(x,y)\n",
        "    # a = cp.ones(x * y).reshape(x,y) * 0.0001\n",
        "    return a\n",
        "\n",
        "driver = lambda x: to_label_list(x)\n",
        "\n",
        "train_list_label_arranged = cp.array(y_train)\n",
        "test_list_label_arranged = cp.array(y_test)\n",
        "\n",
        "train_image = X_train.transpose()\n",
        "test_image = X_test.transpose()\n",
        "\n",
        "def make_mat_list(node_list):\n",
        "    mat_list = []\n",
        "\n",
        "    pre = max_features\n",
        "    for node in node_list:\n",
        "        nex = node \n",
        "        mat_list.append(make_thetas(nex, pre + 1))\n",
        "        pre = nex\n",
        "    mat_list.append(make_thetas(1, pre + 1))\n",
        "\n",
        "    return mat_list\n",
        "\n",
        "from timeit import default_timer as timer\n",
        "s = timer()\n",
        "lr = 0.0693922\n",
        "lamb = 200\n",
        "\n",
        "# test value\n",
        "# lr = 0.6\n",
        "# lamb = 0.01\n",
        "\n",
        "max_acc = 0\n",
        "\n",
        "while True:\n",
        "    import random\n",
        "    \n",
        "    # node_list = []\n",
        "    # layer_num = random.randint(1,2)\n",
        "    # for i in range(layer_num):\n",
        "    #     node_list.append(random.randint(500, 2000))\n",
        "    \n",
        "    # mat_seed = random.randint(1,10000)\n",
        "\n",
        "    # np.random.seed(mat_seed)\n",
        "    node_list = [200]\n",
        "    mats = make_mat_list(node_list)\n",
        "\n",
        "    lamb = random.random() * 150\n",
        "\n",
        "    step_count = 0\n",
        "    \n",
        "    stpcnt_train_costs = {}\n",
        "    stpcnt_test_costs = {}\n",
        "\n",
        "    stpcnt_train_accuracy = {}\n",
        "    stpcnt_test_accuracy = {}\n",
        "\n",
        "    # node_list = [random.randint(100, 1500)]\n",
        "    # np.random.seed(mat_seed)\n",
        "    # mats = make_mat_list(node_list)\n",
        "    # lamb = 10339.976865992260498\n",
        "\n",
        "    max_acc = 0\n",
        "\n",
        "    while True:\n",
        "        train_ret_list = get_ret_list(mats, train_image)\n",
        "        train_cost = calc_energy_value(mats, train_image, train_list_label_arranged, train_ret_list, lamb)\n",
        "\n",
        "        test_ret_list = get_ret_list(mats, test_image)\n",
        "        test_cost = calc_energy_value(mats, test_image, test_list_label_arranged, test_ret_list, lamb)\n",
        "\n",
        "        stpcnt_train_costs[step_count] = float(train_cost)\n",
        "        stpcnt_test_costs[step_count] = float(test_cost)\n",
        "\n",
        "        stpcnt_train_accuracy[step_count] = calc_accuracy(train_ret_list, train_list_label_arranged)\n",
        "        stpcnt_test_accuracy[step_count] = calc_accuracy(test_ret_list, test_list_label_arranged)\n",
        "\n",
        "        \n",
        "        new_lr = lr\n",
        "        \n",
        "        if step_count % 500 == 499:\n",
        "            print(\"loop\", step_count, \"accu\", stpcnt_train_accuracy[step_count], stpcnt_test_accuracy[step_count], \"loss\", stpcnt_train_costs[step_count], stpcnt_test_costs[step_count])\n",
        "            print(f\"max acc refresh. {max_acc} step_count{step_count}\")\n",
        "            pass\n",
        "        \n",
        "        acc = stpcnt_test_accuracy[step_count]\n",
        "        if max_acc < acc:\n",
        "            max_acc = acc\n",
        "\n",
        "        # if step_count > 10000:\n",
        "        #     break\n",
        "\n",
        "        new_mats = calc_next_gradient(new_lr, mats, train_image, train_list_label_arranged, train_ret_list, lamb)\n",
        "        mats = new_mats\n",
        "\n",
        "        if (step_count > 2 and check_convergence(stpcnt_train_costs[step_count-1], stpcnt_train_costs[step_count])):\n",
        "            break\n",
        "\n",
        "        step_count = step_count + 1\n",
        "    \n",
        "    break\n",
        "    # acc = stpcnt_test_accuracy[step_count]\n",
        "    # if max_acc < acc:\n",
        "    #     print(f\"max acc refresh. {acc} node_list{node_list} lamb{lamb} seed{mat_seed} step_count{step_count}\")\n",
        "    #     max_acc = acc\n",
        "    # else:\n",
        "        # print(f\"acc tried. {acc} node_list{node_list} lamb{lamb}\")\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "e = timer()\n",
        "print(\"소요시간\", e - s)\n",
        "print(f\"max acc refresh. {acc} node_list{node_list} lamb{lamb} seed{mat_seed} step_count{step_count}\")\n",
        "# print(list(stpcnt_train_costs.values()))\n",
        "# print(list(stpcnt_test_costs.values()))\n",
        "\n",
        "# 50 ~ ~ 60\n"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loop 499 accu 0.6052819414703783 0.5740432612312812 loss 70.70287731387815 70.71747165615459\n",
            "max acc refresh. 0.5773710482529119 step_count499\n",
            "loop 999 accu 0.6802284082798001 0.6123128119800333 loss 68.21679121704727 68.29549776219103\n",
            "max acc refresh. 0.6139767054908486 step_count999\n",
            "loop 1499 accu 0.7351891506067095 0.6422628951747088 loss 65.89133897664273 66.00577183772965\n",
            "max acc refresh. 0.6439267886855241 step_count1499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-e90621c0d75b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mtrain_ret_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ret_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mtrain_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_energy_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_image\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_list_label_arranged\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ret_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mtest_ret_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ret_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-e90621c0d75b>\u001b[0m in \u001b[0;36mcalc_energy_value\u001b[0;34m(mat_list, f_list, d_list, ret_list, lamb)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# print(d_list[0], ret[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m# print((1 - d_list) * log(1 - ret))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mcalculated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0md_list\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0md_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mcalculated\u001b[0m \u001b[0;34m/=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-54-e90621c0d75b>\u001b[0m in \u001b[0;36mlog\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mfloat_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.00000000000000001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfloat_min\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFELjuKcN1OV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "outputId": "475f8f03-dce1-44b8-a499-6c43cb0cddf6"
      },
      "source": [
        "def ret_to_pred(ret_list):\n",
        "    rets = cp.asnumpy(ret_list[-1][0])\n",
        "    rets[rets<0.5] = 0\n",
        "    rets[rets>=0.5] = 1\n",
        "    return rets\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "\n",
        "y_pred_train = ret_to_pred(train_ret_list)\n",
        "y_pred_test = ret_to_pred(test_ret_list)\n",
        "print(confusion_matrix(y_train, y_pred_train))\n",
        "print(classification_report(y_train,y_pred_train))\n",
        "print(accuracy_score(y_train, y_pred_train))\n",
        "\n",
        "print(confusion_matrix(y_test,y_pred_test))\n",
        "print(classification_report(y_test,y_pred_test))\n",
        "print(accuracy_score(y_test, y_pred_test))\n"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[518 181]\n",
            " [178 524]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.74      0.74       699\n",
            "           1       0.74      0.75      0.74       702\n",
            "\n",
            "    accuracy                           0.74      1401\n",
            "   macro avg       0.74      0.74      0.74      1401\n",
            "weighted avg       0.74      0.74      0.74      1401\n",
            "\n",
            "0.7437544610992148\n",
            "[[195 107]\n",
            " [103 196]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.65      0.65       302\n",
            "           1       0.65      0.66      0.65       299\n",
            "\n",
            "    accuracy                           0.65       601\n",
            "   macro avg       0.65      0.65      0.65       601\n",
            "weighted avg       0.65      0.65      0.65       601\n",
            "\n",
            "0.6505823627287853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCXePlDRrwb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_10_data(ret_list, answer_list, get_correct):\n",
        "    ret_list = cp.asnumpy(ret_list)\n",
        "    for i, l in enumerate(ret_list):\n",
        "        ret_list[i] = cp.asnumpy(l)\n",
        "\n",
        "    answer_list = cp.asnumpy(answer_list)\n",
        "    list_image  = np.empty((size_row * size_col, 10), dtype=float)\n",
        "    list_label  = np.empty(10, dtype=int)\n",
        "\n",
        "\n",
        "    print(answer_list.shape)\n",
        "    i = 0\n",
        "    for idx in range(answer_list.shape[1]):\n",
        "        if i >= 10: \n",
        "            break\n",
        "\n",
        "        output_ret = ret_list[-1]\n",
        "        # print(ret_list)\n",
        "\n",
        "        calculated = np.argmax(ret_list[-1][:, idx], axis=0)\n",
        "        answer_get = np.argmax(answer_list[:, idx], axis=0)\n",
        "        corrected = calculated == answer_get\n",
        "\n",
        "        # print(\"c\", calculated)\n",
        "        # print(\"a\", answer_get)\n",
        "        # print(\"c\", corrected)\n",
        "        if corrected == get_correct:\n",
        "            # print(ret_list)\n",
        "            list_image[:, i] = ret_list[0][:, idx]\n",
        "            list_label[i] = calculated\n",
        "            i = i + 1\n",
        "\n",
        "    return list_image, list_label\n",
        "\n",
        "good_tuple = get_10_data(train_ret_list, train_list_label_arranged, True)\n",
        "bad_tuple = get_10_data(test_ret_list, test_list_label_arranged, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZrSPvbXsORH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def show_classification_example(good_tuple, bad_tuple):\n",
        "    print(\"upper : good classification from train data\")\n",
        "    print(\"under : bad classification from test data\")\n",
        "    f1 = plt.figure(1)\n",
        "\n",
        "    images, labels = good_tuple\n",
        "    for i in range(10):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.title(labels[i])\n",
        "        plt.imshow(images[:,i].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n",
        "\n",
        "        frame   = plt.gca()\n",
        "        frame.axes.get_xaxis().set_visible(False)\n",
        "        frame.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "    f2 = plt.figure(2)\n",
        "\n",
        "    images, labels = bad_tuple\n",
        "    for i in range(10):\n",
        "        plt.subplot(2, 5, i+1)\n",
        "        plt.title(labels[i])\n",
        "        plt.imshow(images[:,i].reshape((size_row, size_col)), cmap='Greys', interpolation='None')\n",
        "\n",
        "        frame   = plt.gca()\n",
        "        frame.axes.get_xaxis().set_visible(False)\n",
        "        frame.axes.get_yaxis().set_visible(False)\n",
        "\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZyNu6sjyQn",
        "colab_type": "text"
      },
      "source": [
        "1. Plot the loss curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYVsAEdVf1IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_plot([(stpcnt_train_costs, \"plot\", \"blue\", \"train\"), (stpcnt_test_costs, \"plot\", \"red\", \"test\") ])\n",
        "show_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5-quCzKjy1s",
        "colab_type": "text"
      },
      "source": [
        "2. Plot the accuracy curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBKNk-0TjvJZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "make_plot([(stpcnt_train_accuracy, \"plot\", \"blue\", \"train\"), (stpcnt_test_accuracy, \"plot\", \"red\", \"test\") ])\n",
        "\n",
        "show_plot()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDIwHcLzorkr",
        "colab_type": "text"
      },
      "source": [
        "3. Plot the accuracy value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mop1GKjpop80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_last(d):\n",
        "    return d[list(d.keys())[-1]]\n",
        "\n",
        "print(f\"train accuracy: {get_last(stpcnt_train_accuracy) * 100}%\")\n",
        "print(f\"test accuracy: {get_last(stpcnt_test_accuracy) * 100}%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQNWUrRypP-q",
        "colab_type": "text"
      },
      "source": [
        "4. Plot the classification example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVinFYHspQ36",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "show_classification_example(good_tuple, bad_tuple)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}